{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageGrab #grabbing image\n",
    "import cv2 #opencv\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#keras imports\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "# from keras.optimizers import SGD , Adam\n",
    "# import keras.optimizers as optimizers\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 99.0.4844\n",
      "Get LATEST chromedriver version for 99.0.4844 google-chrome\n",
      "Driver [/Users/raj/.wdm/drivers/chromedriver/mac64_m1/99.0.4844.51/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "#path variables\n",
    "game_url = \"file:///Users/raj/Documents/DurhamCollege/AI/Semester_2/Capstone-AIDI-2005/train-o-dino/game/index.html\"\n",
    "# game_url = \"chrome://dino\"\n",
    "chrome_driver_path = ChromeDriverManager().install()\n",
    "# chrome_driver_path = '../chromedriver'\n",
    "loss_file_path = \"./objects/loss_df.csv\"\n",
    "actions_file_path = \"./objects/actions_df.csv\"\n",
    "q_value_file_path = \"./objects/q_values.csv\"\n",
    "scores_file_path = \"./objects/scores_df.csv\"\n",
    "\n",
    "#scripts\n",
    "#create id for canvas for faster selection from DOM\n",
    "init_script = \"document.getElementById('game').id = 'game'\"\n",
    "\n",
    "#get image from canvas\n",
    "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
    "return canvasRunner.toDataURL().substring(22)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* Game class: Selenium interfacing between the python and browser\n",
    "* __init__():  Launch the broswer window using the attributes in chrome_options\n",
    "* get_crashed() : return true if the agent as crashed on an obstacles. Gets javascript variable from game decribing the state\n",
    "* get_playing(): true if game in progress, false is crashed or paused\n",
    "* restart() : sends a signal to browser-javascript to restart the game\n",
    "* press_up(): sends a single to press up get to the browser\n",
    "* get_score(): gets current game score from javascript variables.\n",
    "* pause(): pause the game\n",
    "* resume(): resume a paused game if not crashed\n",
    "* end(): close the browser and end the game\n",
    "'''\n",
    "class Game:\n",
    "    def __init__(self,custom_config=True):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self._driver = webdriver.Chrome(executable_path = chrome_driver_path,chrome_options=chrome_options)\n",
    "        self._driver.set_window_position(x=-10,y=0)\n",
    "\n",
    "        self._driver.get(game_url)\n",
    "        self._driver.execute_script(\"Game.OFFSET_SPEED=0.4\")\n",
    "        self._driver.execute_script(init_script)\n",
    "    \n",
    "    def get_crashed(self):\n",
    "        return self._driver.execute_script(\"return Game.instance_.finished\")\n",
    "    def get_playing(self):\n",
    "        return self._driver.execute_script(\"return Game.instance_.running\")\n",
    "    def restart(self):\n",
    "        self._driver.execute_script(\"Game.instance_.restart()\")\n",
    "        self.press_up()\n",
    "    def press_up(self):\n",
    "        self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.SPACE)\n",
    "    def get_score(self):\n",
    "        score = self._driver.execute_script(\"return Game.instance_.offset\")\n",
    "        return math.floor(int(score)*0.1)\n",
    "    # def pause(self):\n",
    "    #     return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
    "    # def resume(self):\n",
    "    #     return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
    "    def end(self):\n",
    "        self._driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoAgent:\n",
    "    def __init__(self,game): #takes game as input for taking actions\n",
    "        self._game = game; \n",
    "        time.sleep(.5)\n",
    "        self.jump()\n",
    "        self.jump()\n",
    "        #to start the game, we need to jump once\n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n",
    "    def jump(self):\n",
    "        self._game.press_up()\n",
    "    def duck(self):\n",
    "        self._game.press_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_sate:\n",
    "    def __init__(self,agent,game):\n",
    "        self._agent = agent\n",
    "        self._game = game\n",
    "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine \n",
    "        self._display.__next__() # initiliaze the display coroutine \n",
    "    def get_state(self,actions):\n",
    "        actions_df.loc[len(actions_df)] = actions[1] # storing actions in a dataframe\n",
    "        score = self._game.get_score() \n",
    "        reward = 0.1*score/10\n",
    "        is_over = False #game over\n",
    "        if actions[1] == 1:\n",
    "            self._agent.jump()\n",
    "        image = grab_screen(self._game._driver) \n",
    "        self._display.send(image) #display the image on screen\n",
    "        if self._agent.is_crashed():\n",
    "            scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
    "            self._game.restart()\n",
    "            time.sleep(.5)\n",
    "            self._agent.jump()\n",
    "            reward = -1\n",
    "            is_over = True\n",
    "        return image, reward, is_over #return the Experience tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "def load_obj(name ):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def grab_screen(_driver = None):\n",
    "    screen =  np.array(ImageGrab.grab(bbox=(40,180,440,400))) #bbox = region of interset on the entire screen\n",
    "    image = process_img(screen)#processing image as required\n",
    "    return image\n",
    "\n",
    "def process_img(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
    "    image = image[:300, :500] #Crop Region of Interest(ROI)\n",
    "    image = cv2.resize(image, (80,80))\n",
    "    return  image\n",
    "\n",
    "def show_img(graphs = False):\n",
    "    \"\"\"\n",
    "    Show images in new window\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        window_title = \"logs\" if graphs else \"game_play\"\n",
    "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)        \n",
    "        imS = cv2.resize(screen, (800, 400)) \n",
    "        cv2.imshow(window_title, screen)\n",
    "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize log structures from file if exists else create new\n",
    "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
    "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
    "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
    "q_values_df =pd.read_csv(actions_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game parameters\n",
    "ACTIONS = 2 # possible actions: jump, do nothing\n",
    "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
    "OBSERVATION = 100. # timesteps to observe before training\n",
    "EXPLORE = 100  # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 16 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "img_rows , img_cols = 80,80\n",
    "img_channels = 4 #We stack 4 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
    "def init_cache():\n",
    "    \"\"\"initial variable caching, done only once\"\"\"\n",
    "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
    "    t = 0\n",
    "    save_obj(t,\"time\")\n",
    "    D = deque()\n",
    "    save_obj(D,\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Call only once to init file structure\n",
    "'''\n",
    "init_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel():\n",
    "    print(\"Now we build the model\")\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(ACTIONS))\n",
    "    adam = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='mse',optimizer=adam)\n",
    "    \n",
    "    #create model file if not present\n",
    "    if not os.path.isfile(loss_file_path):\n",
    "        model.save_weights('model.h5')\n",
    "    print(\"We finish building the model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "main training module\n",
    "Parameters:\n",
    "* model => Keras Model to be trained\n",
    "* game_state => Game State module with access to game environment and dino\n",
    "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
    "'''\n",
    "def trainNetwork(model,game_state,observe=False):\n",
    "    last_time = time.time()\n",
    "    # store the previous observations in replay memory\n",
    "    D = load_obj(\"D\") #load from file system\n",
    "    # get the first state by doing nothing\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] =1 #0 => do nothing,\n",
    "                     #1=> jump\n",
    "    \n",
    "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
    "    \n",
    "\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
    "    \n",
    "\n",
    "    \n",
    "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
    "    \n",
    "    initial_state = s_t \n",
    "\n",
    "    if observe :\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")    \n",
    "    else:                       #We go to training mode\n",
    "        OBSERVE = OBSERVATION\n",
    "        epsilon = load_obj(\"epsilon\") \n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "\n",
    "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
    "    while (True): #endless running\n",
    "        \n",
    "        loss = 0\n",
    "        Q_sa = 0\n",
    "        action_index = 0\n",
    "        r_t = 0 #reward at 4\n",
    "        a_t = np.zeros([ACTIONS]) # action at t\n",
    "        \n",
    "        #choose an action epsilon greedy\n",
    "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
    "            if  random.random() <= epsilon: #randomly explore an action\n",
    "                print(\"----------Random Action----------\")\n",
    "                action_index = random.randrange(ACTIONS)\n",
    "                a_t[action_index] = 1\n",
    "            else: # predict the output\n",
    "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
    "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
    "                action_index = max_Q \n",
    "                a_t[action_index] = 1        # o=> do nothing, 1=> jump\n",
    "                \n",
    "        #We reduced the epsilon (exploration parameter) gradually\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE \n",
    "\n",
    "        #run the selected action and observed next state and reward\n",
    "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
    "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
    "        last_time = time.time()\n",
    "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
    "        \n",
    "        \n",
    "        # store the transition in D\n",
    "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "\n",
    "        #only train if done observing\n",
    "        if t > OBSERVE: \n",
    "            \n",
    "            #sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
    "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
    "\n",
    "            #Now we do the experience replay\n",
    "            for i in range(0, len(minibatch)):\n",
    "                state_t = minibatch[i][0]    # 4D stack of images\n",
    "                action_t = minibatch[i][1]   #This is action index\n",
    "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
    "                state_t1 = minibatch[i][3]   #next state\n",
    "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
    "                \n",
    "\n",
    "                inputs[i:i + 1] = state_t    \n",
    "\n",
    "                targets[i] = model.predict(state_t)  # predicted q values\n",
    "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
    "                \n",
    "                if terminal:\n",
    "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
    "                else:\n",
    "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
    "\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            loss_df.loc[len(loss_df)] = loss\n",
    "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
    "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
    "        t = t + 1\n",
    "        \n",
    "        # save progress every 1000 iterations\n",
    "        if t % 1000 == 0:\n",
    "            print(\"Now we save model\")\n",
    "            game_state._game.pause() #pause game while saving to filesystem\n",
    "            model.save_weights(\"model.h5\", overwrite=True)\n",
    "            save_obj(D,\"D\") #saving episodes\n",
    "            save_obj(t,\"time\") #caching time steps\n",
    "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
    "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
    "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
    "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
    "            q_values_df.to_csv(q_value_file_path,index=False)\n",
    "            with open(\"model.json\", \"w\") as outfile:\n",
    "                json.dump(model.to_json(), outfile)\n",
    "            clear_output()\n",
    "            game_state._game.resume()\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "\n",
    "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
    "\n",
    "    print(\"Episode finished!\")\n",
    "    print(\"************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "def playGame(observe=False):\n",
    "    game = Game()\n",
    "    dino = DinoAgent(game)\n",
    "    game_state = Game_sate(dino,game)    \n",
    "    model = buildmodel()\n",
    "    try:\n",
    "        trainNetwork(model,game_state,observe=observe)\n",
    "    except StopIteration:\n",
    "        game.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c6/m7hdv33j5lv2j9tmr49vnp700000gn/T/ipykernel_83476/288881861.py:18: DeprecationWarning: use options instead of chrome_options\n",
      "  self._driver = webdriver.Chrome(executable_path = chrome_driver_path,chrome_options=chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "We finish building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raj/anaconda3/envs/dino_env_tf/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/Users/raj/anaconda3/envs/dino_env_tf/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-03-26 01:33:48.885267: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps: 1.6592915172594953\n",
      "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 4.761314681472518\n",
      "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 1.6182589968138736\n",
      "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.27352228255166\n",
      "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.956181439175997\n",
      "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.173411900028458\n",
      "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.9906264182394753\n",
      "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.265282567200789\n",
      "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.039428280304833\n",
      "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.176287495743363\n",
      "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.560756199906269\n",
      "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.190130629892627\n",
      "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.9529003741506217\n",
      "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 4.203252725051635\n",
      "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.963928408740802\n",
      "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.998242197367681\n",
      "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.415986523478627\n",
      "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.201644272544135\n",
      "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.76650780950431\n",
      "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.465559975129251\n",
      "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.333039250729869\n",
      "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.277139357945327\n",
      "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.35565035614259\n",
      "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.582010847894602\n",
      "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.121724552357536\n",
      "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.168554836773252\n",
      "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.043755001301544\n",
      "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.371753473488915\n",
      "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.1266922737862854\n",
      "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.360033222833109\n",
      "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 5.093537481055454\n",
      "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 4.936659039763472\n",
      "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 4.425585076075717\n",
      "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 4.741089060226909\n",
      "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.239012868726458\n",
      "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.6254993992514413\n",
      "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.2058995282456424\n",
      "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.1075162883435885\n",
      "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.1723200456793155\n",
      "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.8609318402533264\n",
      "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.077769503139286\n",
      "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.282781993197466\n",
      "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.32643773486037\n",
      "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.199423096754944\n",
      "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.1736236945932905\n",
      "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.0205672480761345\n",
      "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.218675287133772\n",
      "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.173993348366847\n",
      "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.203943642772878\n",
      "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.031740203264185\n",
      "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.379989264896001\n",
      "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.233014921456721\n",
      "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.276829706007113\n",
      "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.273892424733385\n",
      "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 5.010565146961146\n",
      "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.271041808167342\n",
      "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.224019980563263\n",
      "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.279814697659232\n",
      "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.6834855859162996\n",
      "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 4.884429998986851\n",
      "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.230274019991992\n",
      "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.31296665148223\n",
      "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.306019366442997\n",
      "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.246416017365105\n",
      "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.301559073949866\n",
      "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.306112203360249\n",
      "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.2283847813375806\n",
      "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.289689536534379\n",
      "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.123272386596109\n",
      "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.204276543962279\n",
      "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.261884705779546\n",
      "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 5.031995364277342\n",
      "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.29662379915303\n",
      "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.18252600676495\n",
      "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.044144899867905\n",
      "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.270763478556598\n",
      "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.1279838120751995\n",
      "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.242485912508357\n",
      "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.30043001183199\n",
      "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.922802889233276\n",
      "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.002351234779954\n",
      "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.972070620834908\n",
      "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.181212623276521\n",
      "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.271881365599283\n",
      "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 5.241530305946219\n",
      "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.158643630865329\n",
      "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.362091863530709\n",
      "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.1624983501068336\n",
      "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 4.743475084424118\n",
      "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 5.037501080930465\n",
      "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.7280183738698542\n",
      "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 3.88098077878625\n",
      "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "----------Random Action----------\n",
      "fps: 4.913733557641391\n",
      "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.2420010599177145\n",
      "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.224504986145957\n",
      "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.091281442008777\n",
      "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.0808482567654085\n",
      "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.244233158881077\n",
      "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.150840548615448\n",
      "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.164101769863183\n",
      "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.353032995134588\n",
      "TIMESTEP 101 / STATE explore / EPSILON 0.1 / ACTION 1 / REWARD 0.0 / Q_MAX  0 / Loss  0\n",
      "fps: 4.183485506480242\n",
      "TIMESTEP 102 / STATE explore / EPSILON 0.099001 / ACTION 1 / REWARD 0.0 / Q_MAX  13.693852 / Loss  19.98157501220703\n",
      "fps: 0.9921508906238838\n",
      "TIMESTEP 103 / STATE explore / EPSILON 0.098002 / ACTION 1 / REWARD 0.0 / Q_MAX  5.271589 / Loss  13.766754150390625\n",
      "fps: 1.1024143331862328\n",
      "TIMESTEP 104 / STATE explore / EPSILON 0.097003 / ACTION 1 / REWARD 0.0 / Q_MAX  10.196644 / Loss  2.2447574138641357\n",
      "fps: 1.2677580565111917\n",
      "TIMESTEP 105 / STATE explore / EPSILON 0.096004 / ACTION 0 / REWARD 0.0 / Q_MAX  12.162807 / Loss  5.728702545166016\n",
      "fps: 1.2065536949395157\n",
      "TIMESTEP 106 / STATE explore / EPSILON 0.095005 / ACTION 0 / REWARD 0.0 / Q_MAX  14.724675 / Loss  2.8621983528137207\n",
      "fps: 1.1530583659812506\n",
      "TIMESTEP 107 / STATE explore / EPSILON 0.094006 / ACTION 0 / REWARD 0.0 / Q_MAX  16.3463 / Loss  1.6306644678115845\n",
      "fps: 1.0589005450392692\n",
      "TIMESTEP 108 / STATE explore / EPSILON 0.093007 / ACTION 1 / REWARD 0.0 / Q_MAX  20.362268 / Loss  0.019900165498256683\n",
      "fps: 1.1840650064986173\n",
      "TIMESTEP 109 / STATE explore / EPSILON 0.092008 / ACTION 1 / REWARD 0.0 / Q_MAX  23.50614 / Loss  1.6198327541351318\n",
      "----------Random Action----------\n",
      "fps: 1.3103256738675204\n",
      "TIMESTEP 110 / STATE explore / EPSILON 0.091009 / ACTION 0 / REWARD 0.0 / Q_MAX  25.852388 / Loss  1.6904685497283936\n",
      "fps: 1.282589211313733\n",
      "TIMESTEP 111 / STATE explore / EPSILON 0.09001 / ACTION 1 / REWARD 0.0 / Q_MAX  18.71224 / Loss  1.2947229146957397\n",
      "fps: 1.251696636271802\n",
      "TIMESTEP 112 / STATE explore / EPSILON 0.089011 / ACTION 1 / REWARD 0.0 / Q_MAX  22.060188 / Loss  0.5726553797721863\n",
      "fps: 1.1439858498606943\n",
      "TIMESTEP 113 / STATE explore / EPSILON 0.088012 / ACTION 0 / REWARD 0.0 / Q_MAX  31.690212 / Loss  0.1852269321680069\n",
      "fps: 1.2793842806974305\n",
      "TIMESTEP 114 / STATE explore / EPSILON 0.08701300000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  34.899372 / Loss  1.1378793716430664\n",
      "fps: 1.2725525693898956\n",
      "TIMESTEP 115 / STATE explore / EPSILON 0.08601400000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  37.863342 / Loss  1.3296408653259277\n",
      "fps: 1.2629402092041013\n",
      "TIMESTEP 116 / STATE explore / EPSILON 0.08501500000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  41.050327 / Loss  1.3322815895080566\n",
      "fps: 1.1166958422468614\n",
      "TIMESTEP 117 / STATE explore / EPSILON 0.08401600000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  45.53544 / Loss  1.2300894260406494\n",
      "fps: 1.258806807268257\n",
      "TIMESTEP 118 / STATE explore / EPSILON 0.08301700000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  49.685795 / Loss  0.1239086464047432\n",
      "fps: 1.2769712461775842\n",
      "TIMESTEP 119 / STATE explore / EPSILON 0.08201800000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  52.539425 / Loss  0.756966233253479\n",
      "fps: 1.2800851619156792\n",
      "TIMESTEP 120 / STATE explore / EPSILON 0.08101900000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  54.461895 / Loss  0.18327662348747253\n",
      "fps: 1.2738937229290628\n",
      "TIMESTEP 121 / STATE explore / EPSILON 0.08002000000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  55.28927 / Loss  0.4596666693687439\n",
      "fps: 1.1729680933295001\n",
      "TIMESTEP 122 / STATE explore / EPSILON 0.07902100000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  56.682903 / Loss  0.9624706506729126\n",
      "fps: 1.3073588620755328\n",
      "TIMESTEP 123 / STATE explore / EPSILON 0.07802200000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  58.956806 / Loss  1.3769073486328125\n",
      "fps: 1.298194050158533\n",
      "TIMESTEP 124 / STATE explore / EPSILON 0.07702300000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  44.31901 / Loss  0.19984029233455658\n",
      "fps: 1.1378506219473685\n",
      "TIMESTEP 125 / STATE explore / EPSILON 0.07602400000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  63.819324 / Loss  1.2606972455978394\n",
      "fps: 1.1301298423456496\n",
      "TIMESTEP 126 / STATE explore / EPSILON 0.07502500000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  66.26115 / Loss  0.17978307604789734\n",
      "fps: 1.2801062588203123\n",
      "TIMESTEP 127 / STATE explore / EPSILON 0.07402600000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  67.44429 / Loss  1.7141860723495483\n",
      "fps: 1.2187985911314274\n",
      "TIMESTEP 128 / STATE explore / EPSILON 0.07302700000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  68.284836 / Loss  0.2643752098083496\n",
      "fps: 1.2449921933902057\n",
      "TIMESTEP 129 / STATE explore / EPSILON 0.07202800000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  68.95491 / Loss  1.073393702507019\n",
      "fps: 1.2937616153266018\n",
      "TIMESTEP 130 / STATE explore / EPSILON 0.07102900000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  69.81348 / Loss  0.19122231006622314\n",
      "fps: 1.168135874935526\n",
      "TIMESTEP 131 / STATE explore / EPSILON 0.07003000000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  70.23108 / Loss  0.12152606248855591\n",
      "fps: 1.2929468296430195\n",
      "TIMESTEP 132 / STATE explore / EPSILON 0.06903100000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  70.35774 / Loss  0.5904425978660583\n",
      "fps: 1.318960203873701\n",
      "TIMESTEP 133 / STATE explore / EPSILON 0.06803200000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  69.979065 / Loss  0.12398940324783325\n",
      "fps: 1.218259792353171\n",
      "TIMESTEP 134 / STATE explore / EPSILON 0.06703300000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  54.761898 / Loss  0.1542990505695343\n",
      "fps: 1.1423028015982366\n",
      "TIMESTEP 135 / STATE explore / EPSILON 0.06603400000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  68.050476 / Loss  1.864795207977295\n",
      "fps: 1.284285786460861\n",
      "TIMESTEP 136 / STATE explore / EPSILON 0.06503500000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  67.29067 / Loss  0.3117714524269104\n",
      "fps: 1.1854772881953788\n",
      "TIMESTEP 137 / STATE explore / EPSILON 0.06403600000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  66.483864 / Loss  0.14511263370513916\n",
      "fps: 1.2620749005518546\n",
      "TIMESTEP 138 / STATE explore / EPSILON 0.06303700000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  64.72722 / Loss  0.1860276460647583\n",
      "----------Random Action----------\n",
      "fps: 1.1736442229773594\n",
      "TIMESTEP 139 / STATE explore / EPSILON 0.06203800000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  62.75367 / Loss  0.02344939485192299\n",
      "fps: 1.2747168185082518\n",
      "TIMESTEP 140 / STATE explore / EPSILON 0.06103900000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  61.10514 / Loss  0.22620245814323425\n",
      "fps: 1.326692968752224\n",
      "TIMESTEP 141 / STATE explore / EPSILON 0.06004000000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  59.53004 / Loss  0.09474296122789383\n",
      "fps: 1.283460109438103\n",
      "TIMESTEP 142 / STATE explore / EPSILON 0.05904100000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  57.87992 / Loss  0.25050991773605347\n",
      "fps: 1.2663101087637465\n",
      "TIMESTEP 143 / STATE explore / EPSILON 0.05804200000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  56.09212 / Loss  0.1300714612007141\n",
      "fps: 1.1291982116132069\n",
      "TIMESTEP 144 / STATE explore / EPSILON 0.05704300000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  53.679073 / Loss  0.12766586244106293\n",
      "fps: 1.290349415077843\n",
      "TIMESTEP 145 / STATE explore / EPSILON 0.05604400000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  51.301094 / Loss  0.12439417839050293\n",
      "fps: 1.2804032520007522\n",
      "TIMESTEP 146 / STATE explore / EPSILON 0.05504500000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  49.396603 / Loss  4.565080642700195\n",
      "fps: 1.2950682902941142\n",
      "TIMESTEP 147 / STATE explore / EPSILON 0.05404600000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  48.7024 / Loss  2.428598165512085\n",
      "fps: 1.165993271422114\n",
      "TIMESTEP 148 / STATE explore / EPSILON 0.05304700000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  48.78727 / Loss  0.34507906436920166\n",
      "fps: 1.2925623267887085\n",
      "TIMESTEP 149 / STATE explore / EPSILON 0.05204800000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  48.709385 / Loss  2.285656690597534\n",
      "fps: 1.2691582405186166\n",
      "TIMESTEP 150 / STATE explore / EPSILON 0.05104900000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  48.995678 / Loss  2.33292293548584\n",
      "fps: 1.2693061115050295\n",
      "TIMESTEP 151 / STATE explore / EPSILON 0.05005000000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  49.62378 / Loss  0.13728764653205872\n",
      "fps: 1.2871589575197748\n",
      "TIMESTEP 152 / STATE explore / EPSILON 0.04905100000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  49.670425 / Loss  0.12744799256324768\n",
      "fps: 1.160632772964142\n",
      "TIMESTEP 153 / STATE explore / EPSILON 0.04805200000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  49.182343 / Loss  0.4000326097011566\n",
      "fps: 1.2602906511179988\n",
      "TIMESTEP 154 / STATE explore / EPSILON 0.04705300000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  48.773357 / Loss  0.10975363850593567\n",
      "fps: 1.3040141074524259\n",
      "TIMESTEP 155 / STATE explore / EPSILON 0.04605400000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  48.495327 / Loss  0.5248453617095947\n",
      "fps: 1.2946677383787761\n",
      "TIMESTEP 156 / STATE explore / EPSILON 0.04505500000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  48.507137 / Loss  0.060650937259197235\n",
      "fps: 1.12400337015781\n",
      "TIMESTEP 157 / STATE explore / EPSILON 0.04405600000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  48.72158 / Loss  0.10521115362644196\n",
      "fps: 1.2875019146903022\n",
      "TIMESTEP 158 / STATE explore / EPSILON 0.04305700000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  48.849648 / Loss  0.11936622858047485\n",
      "----------Random Action----------\n",
      "fps: 1.3357558208968534\n",
      "TIMESTEP 159 / STATE explore / EPSILON 0.04205800000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  48.127434 / Loss  0.08414695411920547\n",
      "fps: 1.2969027173924808\n",
      "TIMESTEP 160 / STATE explore / EPSILON 0.04105900000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  46.987225 / Loss  0.10215408354997635\n",
      "fps: 1.2976393894421143\n",
      "TIMESTEP 161 / STATE explore / EPSILON 0.04006000000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  46.076195 / Loss  0.14792285859584808\n",
      "fps: 1.1324133749007455\n",
      "TIMESTEP 162 / STATE explore / EPSILON 0.03906100000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  44.983307 / Loss  0.18330219388008118\n",
      "fps: 1.286295657272748\n",
      "TIMESTEP 163 / STATE explore / EPSILON 0.03806200000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  43.784298 / Loss  1.665123462677002\n",
      "fps: 1.2378489904175476\n",
      "TIMESTEP 164 / STATE explore / EPSILON 0.03706300000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  42.84375 / Loss  0.07273197174072266\n",
      "----------Random Action----------\n",
      "fps: 1.2556191169572033\n",
      "TIMESTEP 165 / STATE explore / EPSILON 0.03606400000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  42.08151 / Loss  0.08412351459264755\n",
      "fps: 1.1165076765652602\n",
      "TIMESTEP 166 / STATE explore / EPSILON 0.03506500000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  40.909008 / Loss  0.06462801992893219\n",
      "fps: 1.2658148235668343\n",
      "TIMESTEP 167 / STATE explore / EPSILON 0.03406600000000001 / ACTION 1 / REWARD 0.0 / Q_MAX  39.600636 / Loss  0.08984558284282684\n",
      "fps: 1.2904307985107897\n",
      "TIMESTEP 168 / STATE explore / EPSILON 0.03306700000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  38.5895 / Loss  0.05565853789448738\n",
      "fps: 1.267917483115308\n",
      "TIMESTEP 169 / STATE explore / EPSILON 0.03206800000000001 / ACTION 0 / REWARD 0.0 / Q_MAX  37.770905 / Loss  0.18607798218727112\n",
      "fps: 1.2808005003126954\n",
      "TIMESTEP 170 / STATE explore / EPSILON 0.031069000000000013 / ACTION 0 / REWARD 0.0 / Q_MAX  37.00305 / Loss  0.05430661514401436\n",
      "fps: 1.183517399181869\n",
      "TIMESTEP 171 / STATE explore / EPSILON 0.030070000000000013 / ACTION 1 / REWARD 0.0 / Q_MAX  36.195724 / Loss  0.05897718667984009\n",
      "fps: 1.2946757310035386\n",
      "TIMESTEP 172 / STATE explore / EPSILON 0.029071000000000013 / ACTION 1 / REWARD 0.0 / Q_MAX  35.59585 / Loss  0.05356472730636597\n",
      "fps: 1.2742942975057967\n",
      "TIMESTEP 173 / STATE explore / EPSILON 0.028072000000000014 / ACTION 1 / REWARD 0.0 / Q_MAX  34.683376 / Loss  0.04859576374292374\n",
      "fps: 1.184695431909382\n",
      "TIMESTEP 174 / STATE explore / EPSILON 0.027073000000000014 / ACTION 1 / REWARD 0.0 / Q_MAX  39.397484 / Loss  0.04454971104860306\n",
      "fps: 1.1451146000811951\n",
      "TIMESTEP 175 / STATE explore / EPSILON 0.026074000000000014 / ACTION 0 / REWARD 0.0 / Q_MAX  32.493603 / Loss  0.01044367253780365\n",
      "fps: 1.281912619261343\n",
      "TIMESTEP 176 / STATE explore / EPSILON 0.025075000000000014 / ACTION 0 / REWARD 0.0 / Q_MAX  31.656372 / Loss  0.05888170376420021\n",
      "fps: 1.2950227060245993\n",
      "TIMESTEP 177 / STATE explore / EPSILON 0.024076000000000014 / ACTION 0 / REWARD 0.0 / Q_MAX  30.84283 / Loss  0.4172285497188568\n",
      "fps: 1.2938685748148258\n",
      "TIMESTEP 178 / STATE explore / EPSILON 0.023077000000000014 / ACTION 0 / REWARD 0.0 / Q_MAX  29.98503 / Loss  0.01849578507244587\n",
      "fps: 1.303274636366433\n",
      "TIMESTEP 179 / STATE explore / EPSILON 0.022078000000000014 / ACTION 0 / REWARD 0.0 / Q_MAX  29.068708 / Loss  0.03767186775803566\n",
      "fps: 1.1296957826400913\n",
      "TIMESTEP 180 / STATE explore / EPSILON 0.021079000000000014 / ACTION 1 / REWARD 0.0 / Q_MAX  28.34903 / Loss  0.02849501557648182\n",
      "fps: 1.2959746435014858\n",
      "TIMESTEP 181 / STATE explore / EPSILON 0.020080000000000015 / ACTION 1 / REWARD 0.0 / Q_MAX  27.472162 / Loss  0.029698330909013748\n",
      "fps: 1.2866598626496584\n",
      "TIMESTEP 182 / STATE explore / EPSILON 0.019081000000000015 / ACTION 1 / REWARD 0.0 / Q_MAX  26.463377 / Loss  0.035338014364242554\n",
      "fps: 1.3111744038793445\n",
      "TIMESTEP 183 / STATE explore / EPSILON 0.018082000000000015 / ACTION 0 / REWARD 0.0 / Q_MAX  31.86904 / Loss  0.013527167961001396\n",
      "fps: 1.151955353292887\n",
      "TIMESTEP 184 / STATE explore / EPSILON 0.017083000000000015 / ACTION 0 / REWARD 0.0 / Q_MAX  37.309235 / Loss  1.1951230764389038\n",
      "fps: 1.3004593755270912\n",
      "TIMESTEP 185 / STATE explore / EPSILON 0.016084000000000015 / ACTION 0 / REWARD 0.0 / Q_MAX  23.759094 / Loss  3.3462343215942383\n",
      "fps: 1.2512948351880133\n",
      "TIMESTEP 186 / STATE explore / EPSILON 0.015085000000000015 / ACTION 0 / REWARD 0.0 / Q_MAX  22.777313 / Loss  0.942920982837677\n",
      "fps: 1.3188731086988377\n",
      "TIMESTEP 187 / STATE explore / EPSILON 0.014086000000000015 / ACTION 0 / REWARD 0.0 / Q_MAX  21.895884 / Loss  0.009121490642428398\n",
      "fps: 1.262276965747043\n",
      "TIMESTEP 188 / STATE explore / EPSILON 0.013087000000000015 / ACTION 1 / REWARD 0.0 / Q_MAX  28.55717 / Loss  1.2047030925750732\n",
      "fps: 1.1763321402588864\n",
      "TIMESTEP 189 / STATE explore / EPSILON 0.012088000000000015 / ACTION 0 / REWARD 0.0 / Q_MAX  20.402182 / Loss  0.0061783622950315475\n",
      "fps: 1.3072492532326672\n",
      "TIMESTEP 190 / STATE explore / EPSILON 0.011089000000000016 / ACTION 0 / REWARD 0.0 / Q_MAX  19.769953 / Loss  0.009396743029356003\n",
      "fps: 1.2965078430875545\n",
      "TIMESTEP 191 / STATE explore / EPSILON 0.010090000000000016 / ACTION 0 / REWARD 0.0 / Q_MAX  19.112988 / Loss  0.008644125424325466\n",
      "fps: 1.2994215297984029\n",
      "TIMESTEP 192 / STATE explore / EPSILON 0.009091000000000016 / ACTION 0 / REWARD 0.0 / Q_MAX  18.380236 / Loss  0.011855640448629856\n",
      "fps: 1.1420462002351452\n",
      "TIMESTEP 193 / STATE explore / EPSILON 0.008092000000000016 / ACTION 1 / REWARD 0.0 / Q_MAX  17.7235 / Loss  2.180999279022217\n",
      "fps: 1.2989764411120712\n",
      "TIMESTEP 194 / STATE explore / EPSILON 0.007093000000000016 / ACTION 0 / REWARD 0.0 / Q_MAX  16.771826 / Loss  0.44805875420570374\n",
      "fps: 1.293510250341164\n",
      "TIMESTEP 195 / STATE explore / EPSILON 0.006094000000000016 / ACTION 0 / REWARD 0.0 / Q_MAX  15.909279 / Loss  0.8474860787391663\n",
      "fps: 1.3084947411308643\n",
      "TIMESTEP 196 / STATE explore / EPSILON 0.005095000000000016 / ACTION 0 / REWARD 0.0 / Q_MAX  15.350735 / Loss  0.2003147006034851\n",
      "fps: 1.3027568984581743\n",
      "TIMESTEP 197 / STATE explore / EPSILON 0.004096000000000016 / ACTION 0 / REWARD 0.0 / Q_MAX  14.992959 / Loss  0.032254040241241455\n",
      "fps: 1.1385115840721993\n",
      "TIMESTEP 198 / STATE explore / EPSILON 0.0030970000000000164 / ACTION 1 / REWARD 0.0 / Q_MAX  14.706538 / Loss  0.008566627278923988\n",
      "fps: 1.2753900999925805\n",
      "TIMESTEP 199 / STATE explore / EPSILON 0.0020980000000000165 / ACTION 1 / REWARD 0.0 / Q_MAX  14.739704 / Loss  0.03392591327428818\n",
      "fps: 1.2856640154194934\n",
      "TIMESTEP 200 / STATE explore / EPSILON 0.0010990000000000164 / ACTION 1 / REWARD 0.0 / Q_MAX  14.66018 / Loss  0.03272778540849686\n",
      "fps: 1.2765654903068404\n",
      "TIMESTEP 201 / STATE train / EPSILON 0.00010000000000001631 / ACTION 1 / REWARD 0.0 / Q_MAX  14.458058 / Loss  0.021769143640995026\n",
      "fps: 1.091339596415753\n",
      "TIMESTEP 202 / STATE train / EPSILON -0.0008989999999999838 / ACTION 1 / REWARD 0.0 / Q_MAX  21.248138 / Loss  0.03756996989250183\n",
      "fps: 1.2403649496023668\n",
      "TIMESTEP 203 / STATE train / EPSILON -0.0008989999999999838 / ACTION 0 / REWARD 0.0 / Q_MAX  14.175028 / Loss  0.029908988624811172\n",
      "fps: 1.203596524228444\n",
      "TIMESTEP 204 / STATE train / EPSILON -0.0008989999999999838 / ACTION 0 / REWARD 0.0 / Q_MAX  14.1958275 / Loss  0.4087243676185608\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplayGame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobserve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mplayGame\u001b[0;34m(observe)\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m buildmodel()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtrainNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgame_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43mobserve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserve\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     game\u001b[38;5;241m.\u001b[39mend()\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrainNetwork\u001b[0;34m(model, game_state, observe)\u001b[0m\n\u001b[1;32m     66\u001b[0m     epsilon \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m (INITIAL_EPSILON \u001b[38;5;241m-\u001b[39m FINAL_EPSILON) \u001b[38;5;241m/\u001b[39m EXPLORE \n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#run the selected action and observed next state and reward\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m x_t1, r_t, terminal \u001b[38;5;241m=\u001b[39m \u001b[43mgame_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mlast_time))) \u001b[38;5;66;03m# helpful for measuring frame rate\u001b[39;00m\n\u001b[1;32m     71\u001b[0m last_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mGame_sate.get_state\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m actions[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent\u001b[38;5;241m.\u001b[39mjump()\n\u001b[0;32m---> 14\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mgrab_screen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_game\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display\u001b[38;5;241m.\u001b[39msend(image) \u001b[38;5;66;03m#display the image on screen\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent\u001b[38;5;241m.\u001b[39mis_crashed():\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mgrab_screen\u001b[0;34m(_driver)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_screen\u001b[39m(_driver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 9\u001b[0m     screen \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39marray(\u001b[43mImageGrab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m180\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m440\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m#bbox = region of interset on the entire screen\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     image \u001b[38;5;241m=\u001b[39m process_img(screen)\u001b[38;5;66;03m#processing image as required\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/anaconda3/envs/dino_env_tf/lib/python3.9/site-packages/PIL/ImageGrab.py:35\u001b[0m, in \u001b[0;36mgrab\u001b[0;34m(bbox, include_layered_windows, all_screens, xdisplay)\u001b[0m\n\u001b[1;32m     33\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mcall([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreencapture\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-x\u001b[39m\u001b[38;5;124m\"\u001b[39m, filepath])\n\u001b[1;32m     34\u001b[0m im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(filepath)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m os\u001b[38;5;241m.\u001b[39munlink(filepath)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox:\n",
      "File \u001b[0;32m~/anaconda3/envs/dino_env_tf/lib/python3.9/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "playGame(observe=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 99.0.4844\n",
      "Get LATEST chromedriver version for 99.0.4844 google-chrome\n",
      "Driver [/Users/raj/.wdm/drivers/chromedriver/mac64_m1/99.0.4844.51/chromedriver] found in cache\n",
      "/var/folders/c6/m7hdv33j5lv2j9tmr49vnp700000gn/T/ipykernel_83476/506048108.py:17: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(),\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# import Action chains\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"disable-infobars\")\n",
    "# chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument(\"--start-maximized\") #open Browser in maximized mode\n",
    "chrome_options.add_argument(\"--no-sandbox\") #bypass OS security model\n",
    "# chrome_options.add_argument(\"--disable-dev-shm-usage\") #overcome limited resource problems\n",
    "# chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "# chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(),\n",
    "                          chrome_options=chrome_options)\n",
    "driver.get(game_url);\n",
    "\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "\n",
    "\n",
    "# action = ActionChains(driver)\n",
    "# action.key_down(Keys.SPACE).send_keys(Keys.SPACE).key_up(Keys.SPACE).perform()\n",
    "\n",
    "driver.find_element_by_tag_name(\"body\").send_keys(Keys.SPACE)\n",
    "# driver.find_element_by_tag_name(\"body\").key_down(Keys.SPACE)\n",
    "\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "# import Action chains\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "# import KEYS\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# create webdriver object\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# get geeksforgeeks.org\n",
    "driver.get(\"https://www.geeksforgeeks.org/\")\n",
    "\n",
    "# create action chain object\n",
    "action = ActionChains(driver)\n",
    "\n",
    "# perform the operation\n",
    "action.key_down(Keys.CONTROL).send_keys('F').key_up(Keys.CONTROL).perform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=ChromeDriverManager().install())  # Optional argument, if not specified will search path.\n",
    "driver.get('http://www.google.com/');\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "search_box = driver.find_element_by_name('q')\n",
    "search_box.send_keys('ChromeDriver')\n",
    "search_box.submit()\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
